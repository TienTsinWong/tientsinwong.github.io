<html>
<title>My Favorite Publication</title>
<body background="./icon/desktop.jpg">

<blockquote>

<font face="Verdana">
<h2>My Favorite Works</h2>

<blockquote>
<a href="publication.html">Publication (Recent & Selected)</a><br>
<a href="publication-year.html">Full Publication by Year</a><br>
</blockquote>

<font face="Verdana" size="2">
The titles below may not be exactly the same as in the papers.
They are used  in order to highlight the key ideas/contributions.
Click [<u>more</u>] for details of the paper. Click [<u>paper</u>] to
download the PDF right away. Click [<u>video</u>] for the high-quality project video. 
For moderate quality video on youtube, please click [<u>youtube</u>].
Click [<u>software</u>] for the associated software. Click [<u>press</u>]
for related press clipping.
</font>


<br>
<p>

<! 
  Sorting Criteria:

  There is no specific sorting criteria. I just highlight my major research.

>


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <!--a href="/papers/dynamicrafter/dynamicrafter.html"-->
  <table border =0 cellpadding=0 align = center>
    <tr border=0>
      <td border=0><img width=149 src="papers/dynamicrafter/images/bear.png" border=0></td>
      <td border=0><img width=149 src="papers/dynamicrafter/images/bear.gif" border=0></td>
    </tr>
  </table>
  <br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
   DynamiCrafter: Animating Images with Video Diffusion Priors </b>
   <i><b> (arXiv preprint)</b></i></font><br> 

There has long been a need to animate images, but early works are limited to animating specific
types of motion, such as water or candlelight. However, animating the general motion of an 
arbitrary image remains an open problem. By utilizing motion priors of a text-to-video diffusion
model, we now can animate any still picture with realistic and natural motion.


<br>
<b>
<!--[<a href="https://ttwong12.github.io/papers/dynamicrafter/dynamicrafter.html"more</a>]-->
[<a href="https://doubiiu.github.io/projects/DynamiCrafter/">more</a>]
[<a href="https://arxiv.org/abs/2310.12190">paper</a>]
[<a href="https://github.com/Doubiiu/DynamiCrafter">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <!--a href="./papers/mvd/mvd.html"-->
  <table border =0 cellpadding=0 align = center>
    <tr border=0>
      <td border=0><img width=149 src="papers/mvd/images/lincoln.gif" border=0></td>
      <td border=0><img width=149 src="papers/mvd/images/monroe.gif" border=0></td>
    </tr>
  </table>
  <br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
   Text-Guided Texturing by Synchronized Multi-View Diffusion </b>
   <i><b> (arXiv preprint)</b></i></font><br> 

Texturing a mesh could be very tedious and labor intensive. There is nothing
better than dressing an object by simply typing a few words. By utilizing the 
image prior of a pretrained text-to-image diffusion
model, we can synthesize high-quality textures for any given object mesh.
The key challenge lies on how to produce a high-quality 360&deg;-complete,
harmonized and realistic textures, without
over-fragmentation, obvious seams, and over-blurriness.


<br>
<b>
[<a href="https://github.com/LIU-Yuxin/SyncMVD">more</a>]
[<a href="https://arxiv.org/abs/2311.12891">paper</a>]
[<a href="https://github.com/LIU-Yuxin/SyncMVD">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->



<table border=0><tr><td width=300 height=80 align=right valign=top>

  <!--a href="./papers/mfd/mfd.html"-->
  <img width=300 src="papers/mfd/images/car.gif" border=0></td>

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
   Detailed and Consistent Video Stylization with Multi-Frame Diffusion </b>
   <i><b> (arXiv preprint)</b></i></font><br> 

Video stylization converts a given video into the desired style or effect. 
Existing text-to-image diffusion models offer a convenient and high-quality
style transfer for still picture by just typing a text prompt.  However,
extending to video is challenging to preserve the
temporal consistency (consistency of visual content in all
scales),  high degree of visual detail, and sharpness. By assigning a
diffusion process for each frame and sychronized among them, we are able to
synthesize temporal consistent in all visual scales, rich detailed, and
sharp video stylization.


<br>
<b>
[<!--a href="./papers/mfd/mfd.html"-->more</a>]
[<a href="https://arxiv.org/abs/2311.14343">paper</a>]
[<!--a href=""-->code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="https://doubiiu.github.io/projects/aidn/">
  <img src="papers/invscale/images/icon.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Scale-Arbitrary Invertible Image Downscaling </b>
  <i><b> (IEEE TIP 2023)</b></i></font><br> 
  <p>
  Have you ever complained your image being degraded after posting over
  whatsapp, telegram, or facebook? This is because social media platforms
  usually scale down your image if its resolution is too large. Here,
  we provide a solution of encoding arbitrary high-resolution image details into the
  low-resolution ones, so that after posting the low-resolution images 
  on these social media platforms, we can still restore the high-resolution
  images and their details. Obviously, this technology  can also be 
  adopted by the social media platforms, to retain the high-resolution
  details of users' images without paying extra storage cost.


<br>

<b>
[<a href="https://doubiiu.github.io/projects/aidn/">more</a>]
[<a href="https://ieeexplore.ieee.org/document/10192538">paper</a>]
[<a href="https://github.com/Doubiiu/AIDN">code</a>]

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/codetalker/codetalker.html">
  <img width=300 src="papers/codetalker/images/icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
   CodeTalker: Speech-Driven 3D Facial Animation</b>
   <i><b> (CVPR 2023)</b></i></font><br> 
  <p>

While speech-driven 3D facial animation has been heavily studied, there remains
a gap to achieving expressive and dramatic animation, due to the highly ill-posed
nature and scarcity of audiovisual data.  The unique contribution here is on
minimizing the over-smoothed facial expression using a learned discrete
motion prior, which means more dramatic and expressive facial motions with
more accurate lip movements can be achieved.  Such method is not only useful in virtual reality,
games and film productions, but also is beneficial to general non-professional users
without much animation skill.

<br>
<b>
[<a href="./papers/codetalker/codetalker.html">more</a>]
[<a href="https://doubiiu.github.io/projects/codetalker/">more 2</a>]
[<a href="./papers/codetalker/codetalker.pdf">paper</a>]
[<a href="./papers/codetalker/codetalker.mp4">video</a>]
[<a href="https://github.com/Doubiiu/CodeTalker">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/disco/disco.html">
  <img width=300 src="papers/disco/images/icon.jpg" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Disentangled Image Colorization</b>
  <i><b> (SIGGRAPH Asia 2022)</b></i></font><br> 
  <p>
Existing neural-based colorization methods usually fail to consistently colorize, e.g. the dress of a
person. The typical result is the dress is partly in green and partly in gray
even the grayscale dress looks like in the same color. The core of the
problem lies on the difficulty in maintaining the long-distance color
consistency for region that can be colorized in "any" color. To ease the learning, we propose to disentangle the color
multimoality from the structure consistency. So that, a person wearing the
single color dress will be colorized in single color.

<br>

<b>
[<a href="./papers/disco/disco.html"-->more</a>]
[<a href="https://menghanxia.github.io/projects/disco.html"-->more 2</a>]
[<a href="./papers/disco/disco.pdf">paper</a>]
[<a href="https://www.youtube.com/watch?v=Zb5F0449THA">youtube</a>]
[<a href="https://github.com/MenghanXia/DisentangledColorization">code</a>]
[<a href="https://huggingface.co/spaces/menghanxia/disco">demo</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/dashline/dashline.html">
  <img width=300 src="./papers/dashline/images/icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Dashed Curves Vectorization</b>
  <i><b> (CVPR 2022)</b></i></font><br> 
  <p>
Dashed lines/curves vectorization has never been successfully handled so
far. We take one step further to vectorize dashed curves using an end-to-end neural
approach. Instead of exporting multiple disjoint line segments, we can export a
semantic meaningful vectorized curve that connects all involved dashes. Even the raster input contains multiple intersecting dashed curves,
we are able to identify them correctly and individually.
  

<br>

<b>
[<a href="./papers/dashline/dashline.html"-->more</a>]
[<a href="./papers/dashline/dashline.pdf">paper</a>]
[<a href="./papers/dashline/dashline.mp4">presentation video</a>]
[<a href="https://github.com/hyliu/DashedCurveRecognition">code (in preparation)</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/linevector/linevector.html">
  <img src="./papers/linevector/images/icon.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  End-to-End Line Drawing Vectorization</b>
  <i><b> (AAAI 2022)</b></i></font><br> 
  <p>

Line vectorization has long been an open problem. Many approaches have been
proposed, and they usually involve multiple steps that may be proned to
input violating the assumptions. In this project, we propose a neural model
that can take arbitrary line drawing as input and directly export vectorized
output, without the hassles of unstable intermediate steps.

<br>

<b>
[<a
href="./papers/linevector/linevector.html"-->more</a>]
[<a href="./papers/linevector/linevector.pdf">paper</a>]
[<a href="https://github.com/hyliu/e2eLineVec">code (in preparation)</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/invhalftone/invhalftone.html">
  <img src="./papers/invhalftone/img/icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Invertible Halftoning  </b>
  <i><b> (ICCV 2021)</b></i></font><br> 
  <p>

Once we can store color information in grayscale, the next step is to store
it in halftone. This problem is much harder because of the drastically reduced
solution space of bitonal image (1 bit per pixel). Besides storing the color
information, the binary pattern also serves to preserve the orginal
structural content while avoid to introduce extra visual pattern (blue-noise
property). Also, convolutional neural networks is not good in generating
binary pattern. See how we solve it.


<br>

<b>
[<a href="./papers/invhalftone/invhalftone.html">more</a>]
[<a href="./papers/invhalftone/invhalftone.pdf">paper</a>]
[<a href="./papers/invhalftone/invhalftone_supp.pdf">supplement</a>]
[<a href="./papers/invhalftone/invhalftone_video.mp4">video</a>]
[<a href="https://github.com/MenghanXia/ReversibleHalftoning">code</a>]
[<a href="https://replicate.ai/menghanxia/reversiblehalftoning">demo</a>]

</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/mangainpaint/mangainpaint.html">
  <img src="./papers/mangainpaint/img/icon-mangainpaint.gif" border=0></a><br>
  

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Manga Inpainting
  </b> <i><b> (SIGGRAPH 2021)</b></i></font><br>
  <p>
  Manga inpainting has long been needed by the industry during the localization
  and the conversion to electronic manga. But it is mostly done by hand, due to
  the semantically poor  and  seam-obvious inpainting results of existing
  methods. We proposed the first effective deep learning based method for manga
  inpainting that produces semantically meaningful and screentone seamless
  results. Thanks to our previous screenVAE feature, we can avoid the confusion caused by
  the screentone. <br>

<b>
[<a href="./papers/mangainpaint/mangainpaint.html">more</a>]
[<a
href="./papers/mangainpaint/mangainpaint.pdf">paper</a>]
[<a href="./papers/mangainpaint/mangainpaint_supp.pdf">supplement</a>]
[<a href="https://github.com/msxie92/MangaInpainting">code</a>]
</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/mangarestore/mangarestore.html">
  <img src="./papers/mangarestore/img/icon-mangarestore.gif" border=0></a><br>
  

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Exploiting Alaising for Manga Restoration
  </b> <i><b> (CVPR 2021)</b></i></font><br>
  <p>
  Manga images available on the net are usually contaminated with aliasing artifact due
  to the undersampling during the scanning. This is especially true for
  manga, as regular patterns (screentones) are oftenly presented in manga. 
  In fact, such undesired artifact can be exploited as
  a clue to determine the optimal resolution for restoring (superresolving) the
  manga. 
  <br>

<b>
[<a href="./papers/mangarestore/mangarestore.html">more</a>]
[<a href="./papers/mangarestore/mangarestore.pdf">paper</a>]
[<a href="./papers/mangarestore/mangarestore_supp.pdf">supplement</a>]
[<a href="https://github.com/msxie92/MangaRestoration">code</a>]
</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/bpnet/bpnet.html">
  <img src="./papers/bpnet/img/icon-bpnet.jpg" border=0></a><br>
  

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b>
  Bidirectional Projection Network
  </b> <i><b> (CVPR 2021)</b></i></font><br>
  <p>
  Existing segmentation methods are mostly unidirectional, i.e. utilizing 3D
  for 2D segmentation or vice versa. Obviously 2D and 3D information can
  nicely complement each other in both directions, during the segmentation. This is the goal of
  bidirectional projection network.
  <br>

<b>
[<a href="./papers/bpnet/bpnet.html">more</a>]
[<a href="./papers/bpnet/bpnet.pdf">paper</a>]
[<a href="https://www.youtube.com/watch?v=Wt9J1l_UBaA">youtube</a>]
[<a href="https://github.com/wbhu/BPNet">code</a>]
</b>


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/invbino/invbino.html">
  <img src="papers/invbino/img/icon.png" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Mononizing Binocular Videos </b>
  <i><b> (SIGGRAPH Asia 2020)</b></i></font><br> 
  <p>

Here we present a fully backward compatible solution to represent a
binocular video as an ordinary monocular video.  So that, it can be played
back, compressed with standard video codec, transmitted, just like any
ordinary monocular video.  The only uniqueness is that it can also be
optionally restored back to its original binocular form, whenever
stereoscopic playback is needed. We achieved this by employing the
InvertibleX Model. In addition, compressing our mononized
video even outperforms the state-of-the-art multiview video encoding.


<br>

<b>
[<a href="./papers/invbino/invbino.html">more</a>]
[<a href="https://youtu.be/rbZR_sF9B5E?list=PL3zJztb9e6XVa266_SeX0Dj66Vjy5f720">youtube</a>]
[<a href="./papers/invbino/invbino.pdf">paper</a>]
[<a href="https://github.com/wbhu/Mono3D">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/screenstyle/screenstyle.html">
  <img src="papers/screenstyle/img/icon.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Manga Filling with ScreenVAE </b>
  <i><b> (SIGGRAPH Asia 2020)</b></i></font><br> 
  <p>

While automatic converting color comic to screened manga is doable, translating  a bitonal
screened manga to color comic is never done automatically before. The major 
bottleneck lies on the fundamental difference in characterizing how a region
is filled. While a color can be characterized at a single point, a screentone has
to be characterized by a region. To enable the effective automatic 
transation between the color comic and the screened manga, we propose to
unify such fundamental difference, by introducing an intermediate
representation, ScreenVAE map, which converts the region-wise screentone to
a point-wise representation. 

<br>

<b>
[<a href="./papers/screenstyle/screenstyle.html">more</a>]
[<a href="./papers/screenstyle/screenstyle.pdf">paper</a>]
[<a href="https://github.com/msxie92/ScreenStyle">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/embedframe/embedframe.html">
  <img src="./papers/embedframe/img/icon.gif"  border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Video Snapshot</b> (a real live photo)
  <i><b> (IEEE TPAMI 2021)</b></i></font><br> 
  <p>

While iPhone keeps a short video for each "live photo", we propose a method to
embed a short video into a single frame, in other words, a real live photo. We employ
the InvertibleX Model to first encode the neighboring frames into a
single visualizable frame. Whenever the video is needed, a decoding subnetwork
can be used to expand (restore) it.


<br>

<b>
[<a href="./papers/embedframe/embedframe.html">more</a>]
[<a href="https://youtu.be/H5UA5jH4BaQ">youtube</a>]
[<a href="./papers/embedframe/embedframe.pdf"-->paper</a>]
[<a href="https://github.com/chuchienshu/Video-Snapshot">code</a>]
</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<!--#include virtual="./papers/deepcvdvideo/hu-2019-colorblind-desc.html"-->

<!--#include virtual="./papers/invertgray/xia-2018-invertible-desc.html"-->

<!--#include virtual="./papers/colorize/zhang-2018-two-desc.html"-->

<!--#include virtual="./papers/pixel/han-2018-deep-desc.html"-->

<!--#include virtual="./papers/linelearn/li-2017-deep-desc.html"-->

<!--#include virtual="./papers/pad/kwan-2016-pyramid-desc.html"-->

<!--#include virtual="./papers/colorblind/shen-2016-visual-desc.html"-->

<!--#include virtual="./papers/toontrack/zhu-2016-globally-desc.html"-->

<!--#include virtual="./papers/sketch/liu-2015-closure-desc.html"-->

<p>
<table border=0><tr><td width=300 height=80 align=right valign=top>

  <a href="./papers/3dcel/3dcel.html">
  <img src="papers/3dcel/thumbnail/thumbnail3dcel.gif" border=0></a><br>
 

</td><td width=20>
</td><td align=left valign=top  width=600><font face="Verdana" size="2">

  <font size=+0 face="Arial,Helvetica"><b> 
  Stereoscopizing Cel Animations </b>
  <i><b> (SIGGRAPH Asia 2013)</b></i></font><br> 
  <p>
  While 3D movies are popular nowadays, it is impractical for cel
  animators to hand-draw stereo frames. Geometry modeling cartoon characters
  not only costly, but also requires highly trained skill for lively and
  "organic" presentation of cartoon characters. It would be the best if 
  cel animators remains hand-draw their monocular cels, while leaves our 
  system to automatically turn the cel animations into stereo. 
  
<br>

<b>
[<a href="./papers/3dcel/3dcel.html">more</a>]
[<a href="./papers/3dcel/3dcel.pdf">paper</a>]
[<a href="./papers/3dcel/3dcel_framework.avi">video</a>]
[<a href="./papers/3dcel/3dcel_results.avi">3D video</a>]


</td></tr></table>
<p><br>

<!----------------------------------------------------------------------------------------------------->


<!--#include virtual="./papers/binovision/yang-2012-binocular-desc.html"-->

<!--#include virtual="./papers/gestalt/nan-2011-conjoining-desc.html"-->

<!--#include virtual="./papers/puzzle/xin-2011-making-desc.html"-->

<!--#include virtual="./papers/sumresize/wu-2010-resizing-desc.html"-->

<!--#include virtual="./papers/colormood/wang-2010-data-desc.html"-->

<!--#include virtual="./papers/asciiart/xu-2010-structure-desc.html"-->

<!--#include virtual="./papers/camouflage/chu-2010-camouflage-desc.html"-->

<!--#include virtual="./papers/maze/wan-2009-evolving-desc.html"-->

<!--#include virtual="./papers/flock/xu-2008-animating-desc.html"-->

<!--#include virtual="./papers/incolor/liu-2008-intrinsic-desc.html"-->

<!--#include virtual="./papers/solid/kopf-2007-solid-desc.html"-->

<!--#include virtual="./papers/refilm/zhang-2009-refilming-desc.html"-->




<!------------------------------- Computational Manga   ---------------------------------->
<hr>
<a name="manga">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>Computational Manga</b></font>
</a>
<hr>

<!--#include virtual="./papers/screen/qu-2008-richness-desc.html"-->

<!--#include virtual="./papers/structurehalftone/pang-2008-structure-desc.html"-->

<!--#include virtual="./papers/illusion/chi-2008-self-desc.html"-->

<!--#include virtual="./papers/manga/qu-2006-manga-desc.html"-->

<!--#include virtual="./papers/artifact/wang-2006-deringing-desc.html"-->

<a name="halftoning">
<!--#include virtual="./papers/halftone/wong-1995-halftoning-desc.html"-->
</a>





<!----------------------- Weathering  ------------------------------>
<hr>
<a name="weathering">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>Weathering</b></font>
</a>
<hr>


<!--#include virtual="./papers/gammaton/chen-2005-visual-desc.html"-->

<!--#include virtual="./papers/imperfct/wong-1997-geometry-desc.html"-->

<!--#include virtual="./papers/dust/hsu-1995-simulating-desc.html"-->






<!----------------- Sphere Map ---------------------------->
<hr>
<a name="sphere">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>Sphere Map</b></font>
</a>
<hr>

<!--#include virtual="./papers/isocube/wan-2007-isocube-desc.html"-->

<!--#include virtual="./papers/rhombic/fu-2009-rhombic-desc.html"-->

<a name="sampling">
<!--#include virtual="./papers/hpsamp2/wan-2005-spherical-desc.html"-->
</a>


<table border=0><tr>

<td width=80 height=80 align=right valign=top>

   <a href="./papers/udpoint/udpoints.html">
   <img src="papers/udpoint/icon/lihamm.png" border=0>
   </a>

</td>

<td width=20>

</td><td align=left valign=top width=600><font face="Verdana" size="2">

  <font size=+0 face=Helvetica,Ariel><b>
  Hammersley and Halton Points
  </b> <i><b>(JGT 1997)</b></i></font><br>
  <p>

  With a set of simple equations, we can generate uniformly distributed
  (low-discrepancy) but deterministic sample points over the spherical
  surface. The total number of sample points can be arbitrarily specified.
  All these features make it a neat and handy tool for sphere sampling.

 <br>

<b>
[<a href="./papers/udpoint/udpoints.html">more</a>]
[<a href="./papers/udpoint/udpoint.pdf">paper</a>]
[<a href="./papers/udpoint/udpoint.zip">software</a>]

</b>


</td>

</tr></table>
<p><br>



<!----------------------------------------------------------------------------------------------------->









<!--------------------- Relighting --------------------------------->
<hr>
<a name="relight">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>Relighting and Precomputed Lighting</b></font>
</a>
<hr>

<!--#include virtual="./papers/btftile/leung-2007-tileable-desc.html"-->

<!--#include virtual="./papers/shfit/lam-2006-noise-desc.html"-->

<!--#include virtual="./papers/stereobp2/wu-2006-dense-desc.html"-->

<!--#include virtual="./papers/pca/ho-2005-compressing-desc.html"-->

<!--#include virtual="./papers/plenwave/wong-2003-compression-desc.html"-->

<!--#include virtual="./papers/panoshad/wong-2003-real-desc.html"-->

<!--#include virtual="./papers/illusam2/lin-2002-relighting-desc.html"-->

<!--#include virtual="./papers/plenill2/wong-2002-plenoptic-desc.html"-->

<!--#include virtual="./papers/lightfld/wong-1997-image-desc.html"-->




<!----------------- View Interpolation ---------------------------->
<hr>
<a name="view-interpolation">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>View Interpolation</b></font>
</a>
<hr>

<!--#include virtual="./papers/bspi/fu-2004-binary-desc.html"-->

<!--#include virtual="./papers/triorder/fu-1998-triangle-desc.html"-->










<!--------------------------- GPGPU  ----------------------------------->
<hr>
<a name="gpgpu">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>GPGPU</b></font>
</a>
<hr>

<!--#include virtual="./papers/dwtgpu/wong-2007-discrete-desc.html"-->

<!--#include virtual="./papers/ecgpu/fok-2005-evolutionary-desc.html"-->













<!--------------------- Volume Rendering -------------------------->
<hr>
<a name="volume-rendering">
<font size=+1 face=Helvetica,Ariel color='#656565'><b>Visualization</b></font>
</a>
<hr>

<!--#include virtual="./papers/asc/poston-1998-multiresolution-desc.html"-->

<!--#include virtual="./papers/arthro/pheng-2004-virtual-desc.html"-->





















<br>
</font>


</blockquote>
</HTML>
